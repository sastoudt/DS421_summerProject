
\documentclass[12pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage{color}
% See the ``Article customise'' template for come common customisations

\title{DS421 Summer 2016 Project Summary}
\author{Sara Stoudt}
%\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
%\tableofcontents

\section{Investigation of Chlorophyll}

\subsection{Goals and Approaches}
The overarching goal of this portion of the project is to understand what variables help explain the variation of chlorophyll over time and at each station. We want to understand if these relationships differ over time and/or over space. My approach was to first see which other variables that we have information on at the same times and places as chlorophyll are correlated with chlorophyll. I included these variables in what I refer to as the ``parsimonious" model. I then added to a "full" model additional variables that were identified by David as variables whose relationships with chlorophyll, in context, would be interesting to understand. I build models including these variables separately for each station. Then I experimented with truly spatial models by using the station ID as a factor in a model, so I could fit one model and be able to predict chlorophyll for each station at once. After this I went back to make more ``bare bones" models per station that only incorporate the date (to incorporate trends over time) and the day of year (to incorporate seasonal trends) as well as the interaction between the two, allowing these trends to change depending on the value of the other one in order to have more fair comparisons between the various models. I then moved towards models that incorporate a sense of ``flow" using chlorophyll values from neighboring stations and measures of true flow throughout the area. 

Throughout all of these models, I fitted the models in a nested way so that we could see how much each variable contributed to the overall fitted value. I created a Shiny application (a web application written in R) to be able to compare and contrast the fitted values from each model, the contributions of each variable to the fitted values in each model, the overall RMSE per station of each model plotted on a map, and the contribution of volumetric flow averaged yearly and averaged monthly for each station.

\subsection{Details About the Setup}
I chose to use a log link function in each Generalized Additive Model since the distribution of chlorophyll is highly skewed right. I was deciding between log transforming the response and fitting a linear model ($log(y)=X\beta)+\epsilon$) versus using a Generalized Linear Model using a log link function ($log(y+\epsilon)= X\beta$) The residuals versus fitted values plots all look reasonable for constant variance under the GAM approach, so it did not seem necessary to use the log transform which would affect the variance as well as the linearity. We also don't have to deal with the back-transform bias where the back transform of the mean of the response on a log scale is not equivalent to the mean of the response on the raw scale.

I also had to choose which stations to focus on. I decided to pick stations that had records throughout the widest range of dates (from 1975-2015). These stations 15 stations (out of 41) are: C10, C3, D10, D12, D19, D22, D26, D28A, D4, D41, D6, D7, D8, MD10, and P8. This narrowed down the number of stations to a reasonable number (since I was often fitting a model per station) while still representing a wide spatial range. For a few of these stations, there was sufficient missingness in some of the variables of interest that I just decided to remove that variable from that particular model for that station. For full spatial models, I only used variables that were available across all of the stations.


\subsection{Parsimonious Model}

The parsimonious model used variables that visually looked correlated with chlorophyll overall along with temporal components. The day of year was included to account for any seasonality and was fit with a cyclic spline so that the ends match up from year to year. The date was included to account for any trends over time. The non-time covariates that were looked correlated were pheo (pheophytin a), tn (total nitrogen), and do.per (dissolved oxygen per ?). Note that do.per was more highly correlated than do (dissolved oxygen), but I'm not sure what the ``per" means. If in context, the ``per" variable doesn't make sense, it can be replaced with ordinary dissolved oxygen. All but day of year were fit with thin plate regression splines. These splines have some optimality properties, are low rank (needing to fit many fewer coefficients), and isotropic (rotation of the covariate coordinate system will not change the smoothing). The same model form was fit for each station. The maximum basis dimensions per variable change per station to allow for extra flexibility if need be and subject to the constraints of the data at each station.  

\textcolor{red}{come back and put results/observations}


\subsection{Full Model}

The full model included everything in the parsimonious model and then added some extra variables that are interesting in context but did not look particularly predictive of chlorophyll in pairwise scatterplots. The additional variables that were added are: sio2 (silica), tp (total phosphorus), tss (total suspended solids), nh4 (ammonia), sal (salinity). Again, all of these were fit with thin plate regression splines, the same model form was fit for each station, and the maximum basis dimensions per variable change per station.  Salinity, ammonia, and total nitrogen are the variables that have the most missingness and therefore don't occur in every station's model.

\textcolor{red}{come back and put results/observations}



\subsection{Spatial Models}

The above models were fit per station which did not allow for a true spatio-temporal analysis since each station could not leverage information from the other stations. I tried a few different set-ups for the spatial models in increasing level of complexity.

\begin{enumerate}
\item Spatial Model 1: Day of year and date with station as a factor. Same day of year and date trend, station just acts as an adjustment on the intercept.
\item Spatial Model 2: Day of year and date with station as a factor. Date is allowed to differ by station.
\item Spatial Model 3: Day of year and date with station as a factor. Date and day of year are both allowed to differ by station.
\item Spatial Model 4:  Day of year and date with station as a factor. Date is allowed to differ by station. The interaction between date and day of year is allowed to differ by station. Note that I didn't let day of year to differ by station here to scale back the complexity.
\end{enumerate}

I went back and did Spatial Model 2 [which turned out to be the highest performing] but adding one covariate (pheophytin a, then total nitrogen separately) to see if we could get a boost from a highly correlated covariate from the parsimonious model that we hadn't allowed in a spatial model yet. 

\textcolor{red}{come back and put results/observations}

Now that we are incorporating more data and having many smooths being able to vary across each of 15 stations we have increasing complexity that carries over into the computational costs of fitting these models. 

\subsubsection{Computational Considerations}

I experimented with a few different solutions to the computational problems that came with fitting the spatial models. This really comes down to patience when working on a personal computer; nothing here is extremely computationally intensive, but for my work flow I wanted to have reasonable wait times of a few minutes per model. 

I refreshed my memory on how to take advantage of the Statistical Computing Facility's clusters and figured out how to move files back and forth and submit and monitor progress of various jobs. This is a good solution for fitting models that you are pretty sure of the size of the basis dimensions, but you don't want to wait around for the model to fit. For example, when I wanted to expand Spatial Model 3's basis dimensions to make sure I was making a fair comparison to another model later on in my process, I put it up on the cluster, and came back to get the results after a few hours.

However, in the trial and error phase of choosing basis dimensions, it is still kind of a pain to keep moving files back and forth to check diagnostic plots of various models. For this, I used mainly the ``bam" function in the mgcv package. This is a multiple thread version of the ``gam" function. There was a huge improvement in speed with only using 4 of the maximum 8 threads on my laptop.

I also recently learned how to switch my linear algebra package in R to be the vecLib version of BLAS which can give up to a linear time improvement. 

Once I learned to take advantage of all of these options, the computational complexity of the spatial model fitting was not a major obstacle to progress.

\subsection{Flow Models}

I started this project with a side motivation of thinking about how to incorporate flow information on data that is set up on a network. The network here is a spatial one, the set-up of the stations as compared to the flow of water in the Bay Delta area.

In the spatial models, the station is just a factor. There is no explicit information about which stations are close to one another. However, just considering the Euclidean distance between two stations may not be the most indicative version of distance if the two stations are not flow connected. If we could somehow incorporate information about the flow network structure into the GAM, we may be able to leverage more spatial information than if we merely used longitude and latitude to determine ``closeness".

I had two approaches for trying to incorporate flow that approach the problem from different angles. Using the chlorophyll series of a neighboring station (correlation-wise) as a covariate, incorporates some information about the network that doesn't explicitly use Euclidean distance between stations. \textcolor{red}{A next step would be to do the same thing only using the station that is nearest in a Euclidean sense to see if there are any major differences.}

I also used flow data from http://www.water.ca.gov/dayflow/ as a covariate. Since I used this in a by station approach, this doesn't actually incorporate information about the spatial network, but it does help us think about flow in general. 



\subsubsection{Neighboring (Correlation-Wise) Station Chlorophyll}

My first approach was to compare the chlorophyll series between each station and see which station's chlorophyll are most correlated. Since not every station was measured on the same days, merging the station series by day to get a true correlation left many missing value pairs between stations. Every station had about 600 measurements, so I decided that any correlation made with less than 100 pairwise comparisons that weren't NA were not to be used because they could be unduly high due to small numbers of comparisons. Then each station's ``best match" was merged on a "closest date" criteria to be a covariate for a by station model. This means that even if the chlorophyll measurements were taken a day apart, they would be merged. This was to help with missingness issue, although I admit that the treatment of missigness in this whole workflow is not necessarily the best way of doing things.

\textcolor{red}{come back and put results/observations}


\subsubsection{True Flow}

My second approach was inspired by Marcus Beck's work on adjusting for flow in his Weighted Regression on Time, Discharge, and Season models. The use of flow as a covariate can help account for the variance in the response that is attributed to flow instead of allowing it to be potentially confounded with a temporal trend. \textcolor{To make this approach more spatial, I should go back and fit a spatial model with flow as a covariate allowing as many of the temporal components to vary by station as possible. Then at least the flows from each station can be leveraged.} A variant on the flow approach that I did try was to look at the volumetric flow data, determine which source made the largest contribution on average to each station, and then using flow data matching the largest contribution source from the water flow data. This was not always possible since for example, there was no obvious matching water flow data for the Carquinez Straight at Martinez even though a few stations had this as their highest volumetric component. To better understand flow over time, I  made plots that show the composition of flow at each station aggregated by year to see changes over time, and by month to see changes over seasons. These can be found in the Shiny app.

\textcolor{red}{come back and put results/observations}

\subsection{In Progress}

I am in the process of trying to use the method described in , O' Donnel, Rushworth, Bowman, and Scott's ``Flexible regression models over river networks'' in Journal of the Royal Statistical Society, Applied Statistics, 2014.

The main idea here is to use penalized B-splines that respect the spatial network of the flow. Because it boils down to a spline approach, I thought it might be a nice comparison. However, the current obstacle is to get the data into the format expected by the smnet package that implements this approach. The data goes through extensive GIS preprocessing before being in the right format. I am trying to circumvent this process, drawing out the network by hand and trying to massage everything into the correct format so that we plug into the heart of main functionality of the smnet package. I am going through the source code, and making some progress, but it is not ready yet.


\section{Generalized Additive Model (GAM) and Weighted Regression on Time, Discharge, and Season (WRTDS) Comparison}

\subsection{Goals and Approaches}
The goal of this portion of the project is to mirror the comparison of GAMs and WRTDS done by Marcus Beck and Rachel Murphy (https://github.com/fawda123/patux_manu/blob/jawra/patux_manu.pdf) but provide the same framework in a new environment (Bay Delta). I have mainly gone through Marcus's work here (https://github.com/fawda123/sf_trends) and made the GAM models using the data that he pre-processed. I then built the comparison framework following the manuscript and a Shiny application mirroring (https://beckmw.shinyapps.io/sf_trends/). Throughout, I learned more about the inner workings of the WRTDStidal package and the context of the data from Marcus.

The context of the project is to understand the variability in dissolved inorganic nitrogen, ammonium, and nitrate at many of the same stations considered in the chlorophyll part of the project. Flow is represented by flow records from the San Joaquin River, Sacramento River, or salinity as a tracer of flow depending on the station.

\subsection{Building the GAM Models}

To build these GAM models, I took a less individualistic approach per station because I didn't want the comparison of GAM and WRTDS to be unfairly impacted by fine tuning per station. The response variable was on the log scale to match what was used in the WRTDS models. Comparisons are made on the log scale to avoid any biases with backtransforming.

I started with the default parameters for the predictor variables day of year, date, and flow for each response, allowing for all possible 2 way interactions as well as a full 3-way interaction. The day of year portions were fit with cyclic splines to ensure they matched up at the ends. All others were fit with thin plate regression splines. I also fit the same model but without flow to see how much flow was helping in explaining the variability in each response. 

From checking the diagnostics of these, I got a sense for how much the maximum basis dimensions needed to be expanded. I picked reasonably large basis dimensions per parameter and then fit a consistent model to each station/response variable pair. A few of the models yielded warnings, and then I adjusted the basis dimensions a bit to get the warnings to go away, but keeping the adjustments as consistent with one another as possible. I also fit the models in a nested way so that we could isolate the impact of each interaction above and beyond the individual components. 





\subsection{In Progress}

\end{document}